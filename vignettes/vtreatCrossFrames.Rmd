---
title: "vtreat cross frames"
author: "John Mount, Nina Zumel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vtreat cross frames}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

Example demonstrating "cross validated training frames" (or "cross frames") in vtreat.

Consider the following data frame.

```{r}
set.seed(22626)
d <- data.frame(xBad1=sample(paste('level',1:1000,sep=''),2000,replace=TRUE),
                xBad2=sample(paste('level',1:1000,sep=''),2000,replace=TRUE),
                xBad3=sample(paste('level',1:1000,sep=''),2000,replace=TRUE),
                xGood1=rnorm(2000),
                xGood2=rnorm(2000))
d$y <- rnorm(nrow(d))+0.2*d$xGood1 + 0.3*d$xGood2>0.5
d$rgroup <- sample(c("cal","train","test"),nrow(d),replace=TRUE)  # the random group used for splitting the data set, not a variable.

# devtools::install_github("WinVector/WVPlots")
# library('WVPlots')
plotRes <- function(d,predName,yName,title) {
  print(title)
  tab <- table(truth=d[[yName]],pred=d[[predName]]>0.5)
  print(tab)
  diag <- sum(vapply(seq_len(min(dim(tab))),
                     function(i) tab[i,i],numeric(1)))
  acc <- diag/sum(tab)
#  if(requireNamespace("WVPlots",quietly=TRUE)) {
#     print(WVPlots::ROCPlot(d,predName,yName,title))
#  }
  print(paste('accuracy',acc))
}
```

Bad practice: use same set of data to prepare variable encoding and train a model.

```{r badmixcalandtrain}
dTrain <- d[d$rgroup!='test',,drop=FALSE]
dTest <- d[d$rgroup=='test',,drop=FALSE]
treatments <- vtreat::designTreatmentsC(dTrain,c('xBad1','xBad2','xBad3','xGood1','xGood2'),
                                        'y',TRUE,
  rareCount=0 # Note: usually want rareCount>0, setting to zero to illustrate problem
)
dTrainTreated <- vtreat::prepare(treatments,dTrain,
  pruneSig=c() # Note: usually want pruneSig to be a small fraction, setting to null to illustrate problem
)
m1 <- glm(y~xBad1_catB + xBad2_catB + xBad3_catB + xGood1_clean + xGood2_clean,
          data=dTrainTreated,family=binomial(link='logit'))
print(summary(m1))  # notice low residual deviance

dTrain$predM1 <- predict(m1,newdata=dTrainTreated,type='response')
plotRes(dTrain,'predM1','y','model1 on train')
dTestTreated <- vtreat::prepare(treatments,dTest,pruneSig=c())
dTest$predM1 <- predict(m1,newdata=dTestTreated,type='response')
plotRes(dTest,'predM1','y','model1 on test')
```

Notice above that we see a training accuracy of 98% and a test accuracy of 60%.

Now try a proper calibration/train/test split:

```{r separatecalandtrain}
dCal <- d[d$rgroup=='cal',,drop=FALSE]
dTrain <- d[d$rgroup=='train',,drop=FALSE]
dTest <- d[d$rgroup=='test',,drop=FALSE]
treatments <- vtreat::designTreatmentsC(dCal,c('xBad1','xBad2','xBad3','xGood1','xGood2'),
                                        'y',TRUE,
  rareCount=0 # Note: usually want rareCount>0, setting to zero to illustrate problem
)
dTrainTreated <- vtreat::prepare(treatments,dTrain,
  pruneSig=c() # Note: usually want pruneSig to be a small fraction, setting to null to illustrate problem
)
m1 <- glm(y~xBad1_catB + xBad2_catB + xBad3_catB + xGood1_clean + xGood2_clean,
          data=dTrainTreated,family=binomial(link='logit'))
print(summary(m1))  # notice low residual deviance

dTrain$predM1 <- predict(m1,newdata=dTrainTreated,type='response')
plotRes(dTrain,'predM1','y','model1 on train')
dTestTreated <- vtreat::prepare(treatments,dTest,pruneSig=c())
dTest$predM1 <- predict(m1,newdata=dTestTreated,type='response')
plotRes(dTest,'predM1','y','model1 on test')
```

Notice above that we now see a training and test accuracies of 70%.  We have defeated overfit in two: ways training performance is closer to test performance, and test performance is better.

Below is a more statistically efficient practice: building a cross training frame.  The idea is: consider any trained statistical model (in this case our treatment plan and variable selection plan) as a two-argument function f(A,B).  The first argument is the training data and the second argument is the application data.  In our case f(A,B) is: designTreatmentsC(A)\$prepare(B).

When we use the same data in both places to build our training frame as in TrainTreated = f(TrainData,TrainData) we are not doing a good job simulating the future application of f(,) which will be f(TrainData,FutureData).  To improve the quality of our simulation we can call f(CalibrationData,ExampleData) where CalibrationData and ExampleData are disjoint datasets (as we did in the earlier example) and expect this to be a good imitation of future f(CalibrationData,FutureData).

Another approach is to build a "cross validated" version of f.  We split TrainData a list of 3 disjoint row intervals: Train1,Train2,Train3.  Instead of computing f(TrainData,TrainData) compute f(Train2+Train3,Train1)+f(Train1+Train3,Train2)+f(Train1+Train2,Train3) (where + denotes rbind()).  The idea is this looks a lot like f(TrainData,TrainData) except it has the important property that no row in the right-hand side is ever work on by a model built using that row (the important property future data will have) so we have a good imitation of f(TrainData,FutureData).

The idea is: cross validation is used to simulate future data.  The main thing we are doing different is remembering that we can apply cross validation to any two argument function f(A,B) and not only to functions of the form f(A,B) = buildModel(A)\$scoreData(B).  This formulation is in stacking or super-learning for f(A,B) of the form buildSubModels(A)\$combineModels(B) (see: "General oracle inequalities for model
selection" Charles Mitchell and Sara van de Geer; "On Cross-Validation and Stacking:
Building seemingly predictive models on random data" Claudia Perlich and Grzegorz Swirszcz; "Super Learner" Mark J. van der Laan, Eric C. Polley, and Alan E. Hubbard) and can be used to improve ensemble methods in general.

In fact (though it was developed independently) you can think of vtreat as co-superlearner.  Where in a super-learner the user supplies sub-models and the super-learning framework uses cross-validated data frames to build a quality over-model.   In vtreat the the package builds the initial single-variable models and then provides a cross-frame allowing the user to reliably fit an over-model.


Below is the example cross-run.

```{r crossframes}
dTrain <- d[d$rgroup!='test',,drop=FALSE]
dTest <- d[d$rgroup=='test',,drop=FALSE]
prep <- vtreat::mkCrossFrameCExperiment(dTrain,
                                              c('xBad1','xBad2','xBad3','xGood1','xGood2'),
                                        'y',TRUE,
  rareCount=0 # Note: usually want rareCount>0, setting to zero to illustrate problem
)
treatments <- prep$treatments
dTrainTreated <- prep$crossFrame
m1 <- glm(y~xBad1_catB + xBad2_catB + xBad3_catB + xGood1_clean + xGood2_clean,
          data=dTrainTreated,family=binomial(link='logit'))
print(summary(m1))  # notice low residual deviance

dTrain$predM1 <- predict(m1,newdata=dTrainTreated,type='response')
plotRes(dTrain,'predM1','y','model1 on train')
dTestTreated <- vtreat::prepare(treatments,dTest,pruneSig=c())
dTest$predM1 <- predict(m1,newdata=dTestTreated,type='response')
plotRes(dTest,'predM1','y','model1 on test')
```

