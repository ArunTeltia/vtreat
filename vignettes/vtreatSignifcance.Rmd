---
title: "vtreat significance"
author: "John Mount, Nina Zumel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vtreat significance}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

vtreat::prepare includes a required argument pruneSig that (if not NULL) is used to prune variable.  Obviously significance depends on training set size (so is not an intrinsic property of just the variables) and there issues of bias in the estimate (which vtreat attempts to eliminate by estimating significance on cross-validated or out of sample data). As always there is a question what to set a significance control to.

Our advice is the following pragmatic.  

Use variable filtering on wide datasets (datasets with many columns or variables).  Most machine learning algorithms can not defend themselves against large numbers of noise variables (including those that have cross-validation procedures built in).  Examples are given [here](http://www.win-vector.com/blog/2014/02/bad-bayes-an-example-of-why-you-need-hold-out-testing/).

As an upper bound think of setting pruneSig below 1/numberOfColumns.  Setting  pruneSig to 1/numberOfColumns means that (in expectation) only a constant number of pure noise variables (variables with no actual relation to outcome we are trying to predict) should create columns.  This means (under some assumptions, and in expectation) we expect only a bounded number of noisy columns to be exposed to downstream statistical and machine learning algorithms (which they can presumably handle).

As a lower bound think of what sort of good variables get thrown out at a given setting of pruneSig.  For example suppose our problem is categorization in a data set with n/2 positive examples and n/2 negative examples.   Consider the observed significance of an rare indicator variable that is on k-times in training and is only on for positive instances.  A random variable that is on k-times would achieve this purity with probability 2^(-k), so we expect it to have a -log(significance) in the ballpark of k.  So a pruneSig of 2^(-k) will filter all such variables out (be they good or bad).  Thus if you want levels or indicators that are on only an z-fraction of the time on a training set of size n you want pruneSig >> 2^(-z*n).

Example:

```{r}
signk <- function(n,k) {
  d <- data.frame(y=c(rep(TRUE,n/2),rep(FALSE,n/2)),v=FALSE)
  d[seq_len(k),'v'] <- TRUE
  vtreat::designTreatmentsC(d,'v','y',TRUE,verbose=FALSE)$scoreFrame[1,'sig']
}
d <- data.frame(k=c(1,2,3,4,5,10,20,50,100))
d$sigEst <- -log(vapply(d$k,function(k) signk(1000,k),numeric(1)))
print(d)
```
