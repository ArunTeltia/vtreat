---
title: "Nonlinear Transforms"
author: "John Mount"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Nonlinear Transforms}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 7)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library("vtreat")
set.seed(23255)

have_ggplot = requireNamespace("ggplot2", quietly=TRUE)
if(have_ggplot) {
  library("ggplot2")
}
```


# Introduction

Many people ask about the possibility of non-linear transforms in `vtreat`.  

Roughly our current thinking is that non-linear transformation is sufficiently complicated (and possibly problem domain specific) that we don't want to automatically do this on the user's behalf.  It is something we would prefer to leave to the analyst and is compatible with the "not the last step" principle voiced in [arXiv:1611.09477 stat.AP](https://arxiv.org/abs/1611.09477).  If there is an important non-linear effect that you know how to use, it will still be there after `vtreat` processing for you to use.  (There is the exception of variables that get pruned out based on linear scoring and rare interactions that become un-available due to effects/impact coding; though common interactions remain through the indicator variable encoding.)

That being said we have a few favorite non-linear transforms (in addition to usual `log`, `sqrt`, `sigmoid`, `logit`, and so on) that an analyst or data-scientist may want to use prior to `vtreat` transform, or even after model construction.  We are now making experimental versions of these transforms available in the [`vtreat` version 0.5.30](https://github.com/WinVector/vtreat).

The single variable non-linear transforms are:

 * `gamXform` a general spline based transform based on generalized additive models.
 * `monotoneXformD` a non-parametric monotone transform that enforces a known intended direction of relation.
 * `monotoneXform` a non-parametric monotone transform that chooses between increasing or decreasing.
 
All of these transforms move variables into "`y`-units" and thus can be thought of as more advanced versions of [`y'-aware scaling](http://www.win-vector.com/blog/2016/06/y-aware-scaling-in-context/).  The implementation of each of these transformations takes two columns of data and returns a transform function.

The pattern is we take a input or independent variable variable "`x`" and build a model of the outcome or dependent variable `m = y ~ x`.  We then replace the variable `x` with `x. = predict(m,x)` The new variable "`x.`" is said to be "in `y`-units."  `vtreat::prepare(scale=TRUE)` itself does this with a combination of Bayesian methods, logistic regression, and regression.  The user may want to transform some important variables prior to using `vtreat` to get a clearer view of behavior (though this does introduce both additional issues of potential nested model bias, and require incorporating this variable treatment in later model application).

Let's take a brief look at each of these transforms.

## `gamXform`

The `gamXform` uses `mgcv::gam` to fit a single variable generalized additive model (essentially a spline).  This allows a statistically sound smooth reproduction of fairly arbitrary relations.

`gamXform` can be used for regression as shown below.
 

```{r gam}
set.seed(23255)
d <- data.frame(x=0.2*(1:30))
d$y <- sin(d$x) + 0.1*rnorm(nrow(d))
m <- gamXform(d$x,d$y)
d$pred <- m(d$x)
if(have_ggplot) {
  print(ggplot(data=d,mapping=aes(x=x)) + 
          geom_point(aes(y=y)) +
          geom_line(aes(y=pred),color='blue') +
          ggtitle("gamXform regression example",
                  subtitle = "estimated expected value as a function of x"))
  print(ggplot(data=d,mapping=aes(x=pred,y=y)) + 
          geom_point() +
          geom_abline(color='blue') +
          ggtitle("gamXform regression example",
                  subtitle = "y as a function of prediction"))
}
```

`gamXform` accepts standard generalized linear model families, so it also can be used for classification.

```{r gamc}
set.seed(23255)
d$yC <- (sin(d$x) + rnorm(nrow(d))) >0.0
mC <- gamXform(d$x, d$yC, family= stats::binomial(link='logit'))
d$predC <- mC(d$x)
if(have_ggplot) {
  print(ggplot(data=d,mapping=aes(x=x,y=predC)) +
          geom_line() + 
          ggtitle("predicted probability as a function of x"))
  print(ggplot(data=d,mapping=aes(x=predC,color=yC)) + 
          geom_density() +
          facet_wrap(~yC, scales = 'free_y', ncol=1) +
          ggtitle("gamXform classification example"))
}
print(table(truth=d$yC,pred=d$predC>0.5))
```

`gamXform` is clearly overfitting in the above example as `yC` is not such a clean function of `x`.  We can try to force simpler models by specifying a `k` in `gamXform`.

```{r gamck}
mC <- gamXform(d$x, d$yC, k=3, family= stats::binomial(link='logit'))
d$predC <- mC(d$x)
if(have_ggplot) {
  print(ggplot(data=d,mapping=aes(x=x,y=predC)) +
          geom_line() + 
          ggtitle("predicted probability as a function of x, k=3"))
  print(ggplot(data=d,mapping=aes(x=predC,color=yC)) + 
          geom_density() +
          facet_wrap(~yC, scales = 'free_y', ncol=1) +
          ggtitle("gamXform classification example, k=3"))
}
print(table(truth=d$yC,pred=d$predC>0.5))
```

Be aware that splining variables separately, as we show above, is nowhere near as powerful as building a joint generalized additive model.  Also the process is not necessarily invertible as the spline transform may not be monotone (and hence not one-to-one).  However, it can make sense to pre-prepare variables as above and use this to guide variable selection, using original variables for `vtreat` and then building a final generalized additive model.  We will work on the invertibility issue with our next two transforms.

## `monotoneXformD` 

`monotoneXformD` uses isotonic regression (see `stats::isoreg`) to fit a non parametric model with the single constraint that the relation is increasing (or decreasing if the user sets `decreasing=TRUE`).  This allows the user to enforce a relation that is thought to be true for the underlying ideal distribution to speed up inference from noisy empirical data.  It is also an interesting tool to "polish" or "calibrate" probability estimates returned by classifiers.

Here is an example of its use where there is no signal.

```{r monotoneXformD}
set.seed(36346)
d <- data.frame(x= 10*runif(20), y= rnorm(20))
d$what <- 'y'
dR <- d
dR$what <- 'linearPred'
mR <- lm(y~x,data=d)
dR$y <- predict(mR, newdata= dR)
dL <- d
dL$what <- 'isotonicPred'
m1 <- monotoneXformD(d$x,d$y)
dL$y <- m1(dL$x)
dM <- rbind(dR,dL)
dT <- d
dT$what <- 'theoretical expectation'
dT$y <- 0
dM <- rbind(dR,dL,dT)
if(have_ggplot) {
  ggplot(mapping=aes(x=x,y=y,color=what,linetype=what)) + 
    geom_point(data=d) + 
    geom_line(data=dM) +
    ggtitle("data where there is no relation between x and y")
}
```

And an example where there is some signal.

```{r monotoneXformDs}
set.seed(36346)
d <- data.frame(x= 10*runif(20), y= rnorm(20))
reln <- 0.01*d$x^3
d$y <- d$y + reln
d$what <- 'y'
dR <- d
dR$what <- 'linearPred'
mR <- lm(y~x,data=d)
dR$y <- predict(mR, newdata= dR)
dL <- d
dL$what <- 'isotonicPred'
m1 <- monotoneXformD(d$x,d$y)
dL$y <- m1(dL$x)
dT <- d
dT$what <- 'theoretical expectation'
dT$y <- reln
dM <- rbind(dR,dL,dT)
if(have_ggplot) {
  ggplot(mapping=aes(x=x,y=y,color=what,linetype=what)) + 
    geom_point(data=d) + 
    geom_line(data=dM) +
    ggtitle("data where there is a relation between x and y")
}
```

## `monotoneXform`

The `monotoneXform` is the same as `monotoneXformD` except both increasing and decreasing relations are tried and the best of the two is retained.

```{r monotone}
d <- data.frame(x=c(15,1,-12,NA,5,NaN))
d$y <- -log(d$x)
m <- monotoneXform(d$x,d$y)
d$pred <- m(d$x)

d2 <- data.frame(x=-20:20)
d2$pred <- m(d2$x)
if(have_ggplot) {
  ggplot() + 
    geom_point(data=d,mapping=aes(x=x,y=pred),color='blue',
               shape=0) + 
    geom_point(data=d,mapping=aes(x=x,y=y)) + 
    geom_line(data=d2,mapping=aes(x=x,y=pred),color='blue') +
    geom_point(data=d2,mapping=aes(x=x,y=pred),color='blue',
               shape=3,alpha=0.5) +
    ggtitle("recovered monotone (increasing or decreasing) relation")
}
print(d)
```


## Use with vtreat

To use these transforms with `vtreat` we suggest applying these transforms prior to `vtreat` operations (be it `mkCrossFrame*Experiment`, `designTreatments*`, or `prepare`).  Be aware these treatments do hide a fair number of degrees of freedom, so you may want to use a separate portion of your data for non-linear treatment design.  One advantage of such pre-treatment is variables with significant non-linear interaction should have improved significance estimates in `vtreat`'s score frame summary (though variable pruning is always at best heuristic due to both interactions and even distributional dependencies; please see [here](http://www.win-vector.com/blog/2016/09/variables-can-synergize-even-in-a-linear-model/) for an example).

Roughly the use pattern would be (assuming we have three data set: `dCal`, `dTrain`, and `dTest`):

```{r vdata, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(33463)
dTrain <- data.frame(x1=rnorm(100),x2=rnorm(100))
dTrain$y <- sin(dTrain$x1) + dTrain$x2^3 + rnorm(100)
dCal <- dTrain
dTest <- dTrain
yname <- 'y'
colNamesToGam <- 'x1'
colNamesToMonotone <- 'x2'
varlist <- c(colNamesToGam,colNamesToMonotone)
```

```{r vtreatsteps}
# design the single variable non-linear transforms
# assumes colNamesToGam disjoint from colNamesToMonotone
gamTreatments <- lapply(colNamesToGam,
                        function(vname) {
                          gamXform(dCal[[vname]], dCal[[yname]])
                        })
names(gamTreatments) <- colNamesToGam
monotoneTreatments <- lapply(colNamesToMonotone,
                        function(vname) {
                          monotoneXform(dCal[[vname]], dCal[[yname]])
                        })
names(monotoneTreatments) <- colNamesToMonotone
preTreatments <- c(gamTreatments,monotoneTreatments)

# apply the single variable pre treatments
for(vname in names(preTreatments)) {
  dTrain[[vname]] <- preTreatments[[vname]](dTrain[[vname]])
  dTest[[vname]] <- preTreatments[[vname]](dTest[[vname]])
}

# use vtreat on the transformed data
ce <- mkCrossFrameNExperiment(dTrain,varlist,yname)
sf <- ce$treatments$scoreFrame
modelingVars <- sf$varName[sf$sig<1/nrow(sf)]
dTrainPrepared <- ce$crossFrame[ , c(yname,modelingVars), drop= FALSE]
dTestPrepared <- prepare(ce$treatments, dTest,
                         pruneSig= NULL, varRestriction= modelingVars)
```

# Conclusion

The non-linear treatments demonstrated above are essentially non-parametric and quite powerful.  The monotone treatments represent a very strong inductive bias.  In all cases these transforms should be used sparingly and the data scientist should document what domain assumptions are driving their application.

