<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>vtreat significance • vtreat</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="vtreat significance">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">vtreat</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">1.0.5</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/vtreat.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/SavingTreamentPlans.html">Saving Treatment Plans</a>
    </li>
    <li>
      <a href="../articles/vtreatCrossFrames.html">vtreat cross frames</a>
    </li>
    <li>
      <a href="../articles/vtreatGrouping.html">Grouping Example</a>
    </li>
    <li>
      <a href="../articles/vtreatOverfit.html">vtreat overfit</a>
    </li>
    <li>
      <a href="../articles/vtreatRareLevels.html">vtreat Rare Levels</a>
    </li>
    <li>
      <a href="../articles/vtreatScaleMode.html">vtreat scale mode</a>
    </li>
    <li>
      <a href="../articles/vtreatSignificance.html">vtreat significance</a>
    </li>
    <li>
      <a href="../articles/vtreatSplitting.html">vtreat splitting</a>
    </li>
    <li>
      <a href="../articles/vtreatVariableTypes.html">Variable Types</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="http://www.win-vector.com/">Sponsor: Win-Vector LLC</a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>vtreat significance</h1>
                        <h4 class="author">John Mount, Nina Zumel</h4>
            
            <h4 class="date">2018-05-18</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/WinVector/vtreat//blob/master/vignettes/vtreatSignificance.Rmd"><code>vignettes/vtreatSignificance.Rmd</code></a></small>
      <div class="hidden name"><code>vtreatSignificance.Rmd</code></div>

    </div>

    
    
<p><code><a href="../reference/prepare.html">vtreat::prepare</a></code> includes a required argument <code>pruneSig</code> that (if not NULL) is used to prune variables. Obviously significance depends on training set size (so is not an intrinsic property of just the variables) and there are issues of bias in the estimate (which vtreat attempts to eliminate by estimating significance of complex sub-model variables on cross-validated or out of sample data). As always there is a question of what to set a significance control to.</p>
<p>Our advice is the following pragmatic:</p>
<p>Use variable filtering on wide datasets (datasets with many columns or variables). Most machine learning algorithms can not defend themselves against large numbers of noise variables (including those algorithms that have cross-validation procedures built in). Examples are given <a href="http://www.win-vector.com/blog/2014/02/bad-bayes-an-example-of-why-you-need-hold-out-testing/">here</a>.</p>
<p>As an upper bound think of setting <code>pruneSig</code> below <em>1/numberOfColumns</em>. Setting <code>pruneSig</code> to <em>1/numberOfColumns</em> means that (in expectation) only a constant number of pure noise variables (variables with no actual relation to the outcome we are trying to predict) should create columns. This means (under some assumptions, and in expectation) we expect only a bounded number of noisy columns to be exposed to downstream statistical and machine learning algorithms (which they can presumably handle).</p>
<p>As a lower bound think of what sort of good variables get thrown out at a given setting of <code>pruneSig</code>. For example suppose our problem is categorization in a data set with <em>n/2</em> positive examples and <em>n/2</em> negative examples. Consider the observed significance of a rare indicator variable that is on <em>k</em> times in training and is only on for positive instances. A random variable that is on <em>k</em> times would achieve this purity with probability <span class="math inline">\(2^{-k}\)</span>, so we expect it to have a <em>-log(significance)</em> in the ballpark of <em>k</em>. So a <code>pruneSig</code> of <span class="math inline">\(2^{-k}\)</span> will filter all such variables out (be they good or bad). Thus if you want levels or indicators that are on only a <em>z</em> fraction of the time on a training set of size <em>n</em> you want <code>pruneSig</code> &gt;&gt; <span class="math inline">\(2^{-z*n}\)</span>.</p>
<p>Example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">signk &lt;-<span class="st"> </span><span class="cf">function</span>(n,k) {
  sigTab &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="ot">TRUE</span>,n<span class="op">/</span><span class="dv">2</span>),<span class="kw">rep</span>(<span class="ot">FALSE</span>,n<span class="op">/</span><span class="dv">2</span>)),<span class="dt">v=</span><span class="ot">FALSE</span>)
  sigTab[<span class="kw">seq_len</span>(k),<span class="st">'v'</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
  vtreat<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/vtreat/topics/designTreatmentsC">designTreatmentsC</a></span>(sigTab,<span class="st">'v'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,<span class="dt">verbose=</span><span class="ot">FALSE</span>)<span class="op">$</span>scoreFrame[<span class="dv">1</span>,<span class="st">'sig'</span>]
}
sigTab &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">100</span>))
<span class="co"># If you want to see a rare but perfect indicator of positive class</span>
<span class="co"># that's only on k times out of 1000, this is the lower bound on pruneSig</span>
sigTab<span class="op">$</span>sigEst =<span class="st"> </span><span class="kw">vapply</span>(sigTab<span class="op">$</span>k,<span class="cf">function</span>(k) <span class="kw">signk</span>(<span class="dv">1000</span>,k),<span class="kw">numeric</span>(<span class="dv">1</span>)) 
sigTab<span class="op">$</span>minusLogSig =<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(sigTab<span class="op">$</span>sigEst) <span class="co"># we expect this to be approximately k</span>
<span class="kw">print</span>(sigTab)</code></pre></div>
<pre><code>##     k       sigEst minusLogSig
## 1   1 2.388636e-01    1.431863
## 2   2 9.565153e-02    2.347044
## 3   3 4.119677e-02    3.189395
## 4   4 1.836242e-02    3.997449
## 5   5 8.351092e-03    4.785363
## 6  10 1.863495e-04    8.587887
## 7  20 1.131954e-07   15.994150
## 8  50 2.209988e-17   38.350959
## 9 100 1.952762e-34   77.618649</code></pre>
<p>For a data set with 100 variables (and 1000 rows), you might want to set <code>pruneSig</code> &lt;= 0.01 to limit the number of pure noise variables that enter the model. Note that this value is smaller than the lower bounds given above for <span class="math inline">\(k &lt; 5\)</span>. This means that in a data set of this width and length, you may not be able to detect rare but perfect indicators that occur fewer than 5 times. You would have a chance of using such rare indicators in a <em>catN</em> or <em>catB</em> effects coded variable.</p>
<p>Below we design a data frame with a perfect categorical variable (completely determines the outcome y) where each level occurs exactly 2 times. The individual levels are insignificant, but we can still extract a significant <em>catB</em> effect coded variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3346</span>)
n &lt;-<span class="st"> </span><span class="dv">1000</span>
k &lt;-<span class="st"> </span><span class="dv">4</span>
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">rbinom</span>(n,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">&gt;</span><span class="dv">0</span>)
d<span class="op">$</span>catVarNoise &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">paste0</span>(<span class="st">'lev'</span>,<span class="kw">sprintf</span>(<span class="st">"%03d"</span>,<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">/</span>k))),(k<span class="op">+</span><span class="dv">1</span>))[<span class="dv">1</span><span class="op">:</span>n]
d<span class="op">$</span>catVarPerfect &lt;-<span class="st"> </span><span class="kw">paste0</span>(d<span class="op">$</span>catVar,<span class="kw">substr</span>(<span class="kw">as.character</span>(d<span class="op">$</span>y),<span class="dv">1</span>,<span class="dv">1</span>))
d &lt;-<span class="st"> </span>d[<span class="kw">order</span>(d<span class="op">$</span>catVarPerfect),]
<span class="kw">head</span>(d)</code></pre></div>
<pre><code>##         y catVarNoise catVarPerfect
## 1   FALSE      lev001       lev001F
## 501 FALSE      lev001       lev001F
## 251  TRUE      lev001       lev001T
## 751  TRUE      lev001       lev001T
## 2   FALSE      lev002       lev002F
## 252 FALSE      lev002       lev002F</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">treatmentsC &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/vtreat/topics/designTreatmentsC">designTreatmentsC</a></span>(d,<span class="kw">c</span>(<span class="st">'catVarNoise'</span>,<span class="st">'catVarPerfect'</span>),<span class="st">'y'</span>,<span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] "vtreat 1.0.5 inspecting inputs Fri May 18 08:11:39 2018"
## [1] "designing treatments Fri May 18 08:11:39 2018"
## [1] " have initial level statistics Fri May 18 08:11:39 2018"
## [1] "design var catVarNoise Fri May 18 08:11:39 2018"
## [1] "design var catVarPerfect Fri May 18 08:11:39 2018"
## [1] " scoring treatments Fri May 18 08:11:39 2018"
## [1] "have treatment plan Fri May 18 08:11:39 2018"
## [1] "rescoring complex variables Fri May 18 08:11:39 2018"
## [1] "done rescoring complex variables Fri May 18 08:11:39 2018"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate effect significance (not coefficient significance).</span>
estSigGLM &lt;-<span class="st"> </span><span class="cf">function</span>(xVar,yVar,<span class="dt">numberOfHiddenDegrees=</span><span class="dv">0</span>) {
  d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>xVar,<span class="dt">y=</span>yVar,<span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)
  model &lt;-<span class="st"> </span>stats<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/stats/topics/glm">glm</a></span>(stats<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/stats/topics/formula">as.formula</a></span>(<span class="st">'y~x'</span>),
                      <span class="dt">data=</span>d,
                      <span class="dt">family=</span>stats<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/stats/topics/family">binomial</a></span>(<span class="dt">link=</span><span class="st">'logit'</span>))
  delta_deviance &lt;-<span class="st"> </span>model<span class="op">$</span>null.deviance <span class="op">-</span><span class="st"> </span>model<span class="op">$</span>deviance
  delta_df &lt;-<span class="st"> </span>model<span class="op">$</span>df.null <span class="op">-</span><span class="st"> </span>model<span class="op">$</span>df.residual <span class="op">+</span><span class="st"> </span>numberOfHiddenDegrees
  pRsq &lt;-<span class="st"> </span><span class="fl">1.0</span> <span class="op">-</span><span class="st"> </span>model<span class="op">$</span>deviance<span class="op">/</span>model<span class="op">$</span>null.deviance
  sig &lt;-<span class="st"> </span>stats<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/stats/topics/Chisquare">pchisq</a></span>(delta_deviance, delta_df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)
  sig
}

prepD &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/vtreat/topics/prepare">prepare</a></span>(treatmentsC,d,<span class="dt">pruneSig=</span><span class="kw">c</span>())</code></pre></div>
<p>vtreat produces good variable significances using out of sample simulation (cross frames).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(treatmentsC<span class="op">$</span>scoreFrame[,<span class="kw">c</span>(<span class="st">'varName'</span>,<span class="st">'rsq'</span>,<span class="st">'sig'</span>,<span class="st">'extraModelDegrees'</span>)])</code></pre></div>
<pre><code>##              varName          rsq           sig extraModelDegrees
## 1   catVarNoise_catB 0.0003794619  4.683998e-01               249
## 2 catVarPerfect_catP 0.0001776213  6.198374e-01               473
## 3 catVarPerfect_catB 0.6951868195 1.802669e-211               473</code></pre>
<p>For categorical targets we have in the <code>scoreFrame</code> the <code>sig</code> column is the significance of the single variable logistic regression using the named variable (plus a constant term), and the <code>rsq</code> column is the “pseudo-rsquared” or portion of deviance explained (please see <a href="http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/">here</a> for some notes). For numeric targets the <code>sig</code> column is the significance of the single variable linear regression using the named variable (plus a constant term), and the <code>rsq</code> column is the “rsquared” or portion of variance explained (please see <a href="http://www.win-vector.com/blog/2011/11/correlation-and-r-squared/">here</a>) for some notes).</p>
<p>Signal carrying complex variables can score as signficant, even those composed of rare levels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">glm</span>(y<span class="op">~</span>d<span class="op">$</span>catVarPerfect<span class="op">==</span><span class="st">'lev001T'</span>,<span class="dt">data=</span>d,<span class="dt">family=</span>binomial))</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ d$catVarPerfect == "lev001T", family = binomial, 
##     data = d)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.152  -1.152  -1.152   1.203   1.203  
## 
## Coefficients:
##                                   Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                       -0.06014    0.06334  -0.949    0.342
## d$catVarPerfect == "lev001T"TRUE  13.62620  378.59287   0.036    0.971
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1385.5  on 999  degrees of freedom
## Residual deviance: 1382.6  on 998  degrees of freedom
## AIC: 1386.6
## 
## Number of Fisher Scoring iterations: 12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estSigGLM</span>(prepD<span class="op">$</span>catVarPerfect_catB,prepD<span class="op">$</span>y,<span class="dv">0</span>) <span class="co"># wrong est</span></code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## [1] 2.958641e-303</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estSigGLM</span>(prepD<span class="op">$</span>catVarPerfect_catB,prepD<span class="op">$</span>y,
          <span class="dt">numberOfHiddenDegrees=</span><span class="kw">length</span>(<span class="kw">unique</span>(d<span class="op">$</span>catVarPerfect))<span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## [1] 3.963376e-90</code></pre>
<p>Noise variables (those without a relation to outcome) are also scored correctly as long was we account for the degrees of freedom.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">glm</span>(y<span class="op">~</span>d<span class="op">$</span>catVarNoise<span class="op">==</span><span class="st">'lev001'</span>,<span class="dt">data=</span>d,<span class="dt">family=</span>binomial))</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ d$catVarNoise == "lev001", family = binomial, 
##     data = d)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.177  -1.154  -1.154   1.201   1.201  
## 
## Coefficients:
##                               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                   -0.05624    0.06340  -0.887    0.375
## d$catVarNoise == "lev001"TRUE  0.05624    1.00201   0.056    0.955
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1385.5  on 999  degrees of freedom
## Residual deviance: 1385.5  on 998  degrees of freedom
## AIC: 1389.5
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estSigGLM</span>(prepD<span class="op">$</span>catVarNoise_catB,prepD<span class="op">$</span>y,<span class="dv">0</span>) <span class="co"># wrong est</span></code></pre></div>
<pre><code>## [1] 1.223667e-63</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estSigGLM</span>(prepD<span class="op">$</span>catVarNoise_catB,prepD<span class="op">$</span>y,
          <span class="dt">numberOfHiddenDegrees=</span><span class="kw">length</span>(<span class="kw">unique</span>(d<span class="op">$</span>catVarNoise))<span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] 0.07074029</code></pre>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by John Mount, Nina Zumel.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
