---
title: "vtreat on Spark"
output: html_document
---

```{r setup}
library("vtreat")
packageVersion("vtreat")
library("rquery")
packageVersion("rquery")
packageVersion("sparklyr")


db <- sparklyr::spark_connect(version='2.2.0', 
                              master = "local")
db_opts <- rq_connection_tests(db)
options(db_opts)
```

```{r data, include = FALSE}
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
               header=TRUE, sep='\t', na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt', sep=''),
                    header=FALSE, sep='\t')
d$churn = churn$V1
rq_copy_to(db, "kdd2009", d,
           overwrite = TRUE,
           temporary = TRUE)
rm(list = c("dir", "d", "churn"))
```

```{r start}
tmp_name_gen <- wrapr::mk_tmp_name_source("kddvtreat")

d <- db_td(db, "kdd2009") %.>%
  extend_nse(., sample_col = rand()) %.>%
  materialize(db, ., table_name = tmp_name_gen())

y_name <- "churn"
vars <- setdiff(column_names(d), c(y_name, "sample_col"))

d <- d %.>% 
  extend_nse(.,
             is_train = sample_col <= 0.8,
             is_treat = (sample_col > 0.8) & (sample_col <= 0.9),
             is_test = sample_col > 0.9)

d_train <- d %.>%
  select_rows_nse(., sample_col <= 0.8)
d_train <- d %.>%
  select_rows_nse(., sample_col > 0.9)
d_treat <- d %.>%
  select_rows_nse(., (sample_col > 0.8) & (sample_col <= 0.9)) %.>%
  execute(db, .)
```

```{r treatdata}
cl = parallel::makeCluster(4)
tp <- vtreat::designTreatmentsC(d_treat, vars, y_name, 1, parallelCluster = cl)
newvars <- tp$scoreFrame$varName[tp$scoreFrame$sig < 1/nrow(tp$scoreFrame)]
# not working yet, fails at next line.  have a related failing test in rquery
# can start debugging here after we get rquery test cleared.
d_train <- materialize_treated(db, rqplan, d_train, 
                               tmp_name_gen(), temporary = TRUE, overwrite = TRUE)
```




```{r cleanup, include = FALSE}
parallel::stopCluster(cl)
for(ti in tmp_name_gen(dumpList = TRUE)) {
  rq_remove_table(db = db, table_name = ti)
}
sparklyr::spark_disconnect(db)
```
