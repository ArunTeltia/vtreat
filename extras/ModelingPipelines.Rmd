---
title: "Modeling Pipelines"
output: github_document
---

Reusable modeling pipelines are a practical idea that gets re-developed many times in many contexts.  [`wrapr`](https://github.com/WinVector/wrapr)  supplies a particularly powerful pipeline notation and as of version `1.7.1` pipeline re-use system (notes [here](https://winvector.github.io/wrapr/articles/Function_Objects.html)).  We will demonstrate this with the [`vtreat`](https://github.com/WinVector/vtreat) data preparation system.

```{r setup}
library("wrapr")
library("vtreat")
library("glmnet")
library("WVPlots")

# function to make practice data
mk_data <- function(nrows, n_var_cols, n_noise_cols) {
  d <- data.frame(y = rnorm(nrows))
  for(i in seq_len(n_var_cols)) {
    vari = paste0("var_", sprintf("%03g", i))
    d[[vari]] <- rnorm(nrows)
    d$y <- d$y + (2/n_var_cols)*d[[vari]]
    d[[vari]][d[[vari]]>abs(2*rnorm(nrows))] <- NA
    d[[vari]] <- rlnorm(1, meanlog=10, sdlog = 10)*d[[vari]]
  }
   for(i in seq_len(n_noise_cols)) {
    vari = paste0("noise_", sprintf("%03g", i))
    d[[vari]] <- rnorm(nrows)
    d[[vari]][d[[vari]]>abs(2*rnorm(nrows))] <- NA
    d[[vari]] <- rlnorm(1, meanlog=10, sdlog = 10)*d[[vari]]
   }
  d
}

set.seed(2018)
d <- mk_data(10000, 10, 200)
is_train <- runif(nrow(d))<=0.5
dTrain <- d[is_train, , drop = FALSE]
dTest <- d[!is_train, , drop = FALSE]
outcome_name <- "y"
vars <- setdiff(colnames(dTrain), outcome_name)
```

Suppose our analysis plan is the following:

 * Fix missing values with `vtreat`.
 * Scale and center the data.
 * Model `y` as a function of the other columns using `glmnet`.

Now both `vtreat` and `glmnet` can scale, but we are going to keep the scaling
as a separate step to show how composite data preparation pipelines work.

This can be done as follows.

```{r model1}
# TODO: parallle on mkCrossFrameNExperiment?
# TODO: search for alpha?

# design a treatment plan using cross-validation methods
cp <- vtreat::mkCrossFrameNExperiment(dTrain, vars, outcome_name)

# get the list of new variables
sf <- cp$treatments$scoreFrame
newvars <- sf$varName[sf$sig <= 1/nrow(sf)]
print(newvars)

# learn a centering and scaling of the cross-validated 
# training frame
tfs <- scale(cp$crossFrame[, newvars, drop = FALSE], 
             center = TRUE, scale = TRUE)
centering <- attr(tfs, "scaled:center")
scaling <- attr(tfs, "scaled:scale")

# apply the centering and scaling to the cross-validated 
# training frame
tfs <- scale(cp$crossFrame[, newvars, drop = FALSE],
             center = centering,
             scale = scaling)

model <- cv.glmnet(as.matrix(tfs), 
                   cp$crossFrame[[outcome_name]],
                   alpha = 0.5,
                   family = "gaussian", 
                   standardize = FALSE)

# need this function for later
get_column <- function(d, cname) {
  as.numeric(d[, cname, drop=TRUE])
}

pipeline <-
  new("PartialNamedFn",
      fn_name = 'prepare',
      fn_package = "vtreat",
      arg_name = "dframe", 
      args = list(treatmentplan = cp$treatments,
                  varRestriction = newvars)) %.>%
  new("PartialNamedFn",
      fn_name ='subset',
      fn_package = "base",
      arg_name = "x",
      args = list(select = newvars))  %.>%
  new("PartialNamedFn",
      fn_name ='scale',
      fn_package = "base",
      arg_name = "x",
      args = list(center = centering,
                  scale = scaling))  %.>%
  new("PartialNamedFn",
      fn_name ="predict.cv.glmnet",
      fn_package = "glmnet",
      arg_name = "newx",
      args = list(object = model,
                  s = "lambda.1se"))  %.>%
  new("PartialFunction",
      fn = get_column,
      arg_name = "d",
      args = list(cname = "1"))

cat(format(pipeline))

dTrain$prediction <- dTrain %.>% pipeline

WVPlots::ScatterHist(dTrain, "prediction", "y", "fit on training data",
                     smoothmethod = "identity",
                     estimate_sig = TRUE,
                     point_alpha = 0.1,
                     contour = TRUE)

dTest$prediction <- dTest %.>% pipeline

WVPlots::ScatterHist(dTest, "prediction", "y", "fit on test",
                     smoothmethod = "identity",
                     estimate_sig = TRUE,
                     point_alpha = 0.1,
                     contour = TRUE)
```

Of course, using `wrapr`'s sequencing controls on a non-trivial data processing pipeline starts duplicating functionality already present in [`rquery`](https://github.com/WinVector/rquery)/[`rqdatatable`](https://github.com/WinVector/rqdatatable/).  Let's work with `rqdatatable` instead.

What we want is to wrap partially evaluated functions as `rquery` pipeline nodes. While they are more opaque than the `pipe_list()` wrapping they include a number of powerful operators.

```{r rq}
library("rqdatatable")


ops <- mk_td("d", colnames(dTrain)) %.>%
  rq_partial(., 'prepare',
             fn_package = "vtreat",
             arg_name = "dframe", 
             args = list(treatmentplan = cp$treatments,
                         varRestriction = newvars),
             columns_produced = newvars)  %.>%
  select_columns(., newvars) %.>%
  rq_partial(., 'scale',
             arg_name = "x", 
             args = list(center = centering,
                         scale = scaling),
             check_result_details = FALSE) %.>%
  rq_partial(.,
             "predict.cv.glmnet",
             fn_package = "glmnet",
             arg_name = "newx",
             args = list(object = model,
                         s = "lambda.1se"),
             check_result_details = FALSE)  %.>%
  rq_partialf(.,
              get_column,
              arg_name = "d",
              args = list(cname = "1"),
              check_result_details = FALSE) 

cat(format(ops))

dTest %.>% ops %.>% head

head(dTest$prediction)
```

In the above example we are somewhat fighting `rquery` as `rquery` is intended to work only on `data.frame`s and `data.table`s, and in this example we are pushing around matrices and vectors (the `check_result_details = FALSE` settings are turning off checks that usually enforce types and columns.

But we now have two examples of a non-trivial modeling workflow saved as a serialiazble object (alternately `pipeline` and `ops`).
