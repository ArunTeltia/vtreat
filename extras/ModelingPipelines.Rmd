---
title: "Modeling Pipelines"
output: github_document
---

Reusable modeling pipelines are a practical idea that gets re-developed many times in many contexts.  [`wrapr`](https://github.com/WinVector/wrapr)  supplies a particularly powerful pipeline notation and as of version `1.7.1` pipeline re-use system (notes [here](https://winvector.github.io/wrapr/articles/Function_Objects.html)).  We will demonstrate this with the [`vtreat`](https://github.com/WinVector/vtreat) data preparation system.

```{r setup}
library("wrapr")
library("vtreat")
library("glmnet")
library("WVPlots")

ncore <- parallel::detectCores()
cl <- parallel::makeCluster(ncore)
library("doParallel")
registerDoParallel(cl)

# function to make practice data
mk_data <- function(nrows, n_var_cols, n_noise_cols) {
  d <- data.frame(y = rnorm(nrows))
  for(i in seq_len(n_var_cols)) {
    vari = paste0("var_", sprintf("%03g", i))
    d[[vari]] <- rnorm(nrows)
    d$y <- d$y + (2/n_var_cols)*d[[vari]]
    d[[vari]][d[[vari]]>abs(2*rnorm(nrows))] <- NA
    d[[vari]] <- rlnorm(1, meanlog=10, sdlog = 10)*d[[vari]]
  }
  for(i in seq_len(n_noise_cols)) {
    vari = paste0("noise_", sprintf("%03g", i))
    d[[vari]] <- rnorm(nrows)
    d[[vari]][d[[vari]]>abs(2*rnorm(nrows))] <- NA
    d[[vari]] <- rlnorm(1, meanlog=10, sdlog = 10)*d[[vari]]
  }
  d
}

set.seed(2018)
d <- mk_data(10000, 10, 200)
is_train <- runif(nrow(d))<=0.5
dTrain <- d[is_train, , drop = FALSE]
dTest <- d[!is_train, , drop = FALSE]
outcome_name <- "y"
vars <- setdiff(colnames(dTrain), outcome_name)
```

Suppose our analysis plan is the following:

* Fix missing values with `vtreat`.
* Scale and center the data.
* Model `y` as a function of the other columns using `glmnet`.

Now both `vtreat` and `glmnet` can scale, but we are going to keep the scaling
as a separate step to show how composite data preparation pipelines work.

First we combine the pre-processing steps, and a fit model as follows.

```{r model1}
# design a treatment plan using cross-validation methods
cp <- vtreat::mkCrossFrameNExperiment(
  dTrain, vars, outcome_name,
  parallelCluster = cl)

# get the list of new variables
sf <- cp$treatments$scoreFrame
newvars <- sf$varName[sf$sig <= 1/nrow(sf)]
print(newvars)

# learn a centering and scaling of the cross-validated 
# training frame
tfs <- scale(cp$crossFrame[, newvars, drop = FALSE], 
             center = TRUE, scale = TRUE)
centering <- attr(tfs, "scaled:center")
scaling <- attr(tfs, "scaled:scale")

# apply the centering and scaling to the cross-validated 
# training frame
tfs <- scale(cp$crossFrame[, newvars, drop = FALSE],
             center = centering,
             scale = scaling)

# build a cross-validation strategy to help us
# search for a good alph hyper-parameter value
cplan <- vtreat::kWayStratifiedY(
  nrow(dTrain), 5, dTrain, dTrain[[outcome_name]])
# convert the plan to cv.glmnet group notation
foldid <- numeric(nrow(dTrain))
for(i in seq_len(length(cplan))) {
  cpi <- cplan[[i]]
  foldid[cpi$app] <- i
}

# search for best cross-validated alpha
alphas <- seq(0, 1, by=0.05)
cross_scores <- vapply(
  alphas,
  function(alpha) {
    model <- cv.glmnet(as.matrix(tfs), 
                       cp$crossFrame[[outcome_name]],
                       alpha = alpha,
                       family = "gaussian", 
                       standardize = FALSE,
                       foldid = foldid, 
                       parallel = TRUE)
    index <- which(model$lambda == model$lambda.1se)[[1]]
    score <- model$cvm[[index]]
  }, numeric(1))
best_i <- which(cross_scores==min(cross_scores))[[1]]
alpha <- alphas[[best_i]]
print(alpha)

# re-fit model with chosen alpha
model <- cv.glmnet(as.matrix(tfs), 
                   cp$crossFrame[[outcome_name]],
                   alpha = alpha,
                   family = "gaussian", 
                   standardize = FALSE,
                   nfolds = 5, 
                   parallel = TRUE)
```

The question then is: how do we share such a model?  Roughly we need to share the model, any fit parameters (such as centering and scaling choices), *and* the code sequence to apply all of these steps in the proper order.

A not quite satisfactory way to do this is to wrap all of the steps into a function which would then capture all of the needed parameters (`cp$treatments`, `newvars`, `centering`, `scaling`, and `model`) in its [closure](https://en.wikipedia.org/wiki/Closure_(computer_programming)).

```{r}
fn <- function(.) {
  . %.>%
    vtreat::prepare(cp$treatments, ., varRestriction = newvars) %.>%
    subset(., select = newvars) %.>%
    scale(., center = centering, scale = scaling) %.>%
    glmnet::predict.cv.glmnet(model, newx = .,  s = "lambda.1se") %.>%
    .[, "1", drop = TRUE]
}

fn(dTrain) %.>% head(.)

print(fn)
environment(fn)
```

It turns out this does not quite work, which is probably among the reasons this is not a common method of sharing data processing pipelines.

If we were to try to share such a function by saving it as follows.

```{r sv1, eval = FALSE}
saveRDS(fn, "fn.RDS")
saveRDS(dTrain, "dTrain.RDS")
```

And then if we read that function back in into a new `R` session and tried to use it, we would see the following error.

```{r rd1, eval=FALSE}
# Fresh R session, not part of this markdown
library("wrapr")
fn <- readRDS("fn.RDS")
dTrain <- readRDS("dTrain.RDS")
fn(dTrain)
#  Error in vtreat::prepare(cp$treatments, ., varRestriction = newvars) : 
#   object 'cp' not found 
```

`R`'s serialization rules do not save the `R_GlobalEnv` environment.  One can work around this by introducing an intermediate environment, but using closures to store data is a bit problematic.  For instance a *lot* of extra stuff tends to leak as was noted [here](http://www.win-vector.com/blog/2014/05/trimming-the-fat-from-glm-models-in-r/).

We can identify the problem ahead of time by analyzing `fn`.

```{r codetools}
codetools::findGlobals(fn, merge = FALSE)
```

One could then build a custom closure containing the free-variable references.

```{r ctfix}
env <- new.env(parent = globalenv())
for(nmi in codetools::findGlobals(fn, merge = FALSE)$variables) {
  assign(nmi, get(nmi), envir = env)
}
environment(fn) <- env
```

This augmented function can be serialized and works when restored, as it has a private environment that is copied with it.

We want a more explicit solution, that shows later code readers exactly what is going on (even if we have to be a bit more verbose). The idea is: when working with others (or even your future self) you do not want things to have worked due to "luck" or objects that happened to volunteer to join the calculation from your environment.  You want to explicitly declare what is in a calculation, so you can see the deceleration later when skimming the code.  "Works on my machine" is the bane of software development, and "alien artifacts" (items of unknown origin and provenance) hiding in a modeling pipeline undermines reproducible research.  Failures are much cheaper during development than in production, so you want to front-load them and find potential issues early.


Such a solution is given by `wrapr`'s ["function object" abstraction](https://winvector.github.io/wrapr/articles/Function_Objects.html), which treats names of functions, plus arguments as an efficient notation for partial evaluation.  We can use this system to encode our model prediction pipeline as follows.

```{r}
pipeline <-
  pkgfn("vtreat::prepare",
        arg_name = "dframe", 
        args = list(treatmentplan = cp$treatments,
                    varRestriction = newvars)) %.>%
  pkgfn("subset",
        arg_name = "x",
        args = list(select = newvars))  %.>%
  pkgfn("scale",
        arg_name = "x",
        args = list(center = centering,
                    scale = scaling))  %.>%
  pkgfn("glmnet::predict.cv.glmnet",
        arg_name = "newx",
        args = list(object = model,
                    s = "lambda.1se"))  %.>%
  srcfn(".[, cname, drop = TRUE]",
        arg_name = ".",
        args = list(cname = "1"))

cat(format(pipeline))

pipeline@items

str(pipeline@items[[3]])

dTrain %.>% pipeline %.>% head(.)
```

This pipeline is just a list of steps and values with some extra class annotations.  The pipeline can be saved, and contains the required parameters in lists.

```{r eval=FALSE}
saveRDS(pipeline, "pipeline.RDS")
```

And this time the processing pipeline can be read back and used as follows.

```{r eval=FALSE}
# Fresh R session , not part of this markdown
library("wrapr")

p <- readRDS("pipeline.RDS")
dTrain <- readRDS("dTrain.RDS")
dTrain %.>% pipeline %.>% head(.)
```
```{r echo=FALSE}
# simulate reading back for presentation, to make sure values match
dTrain %.>% pipeline %.>% head(.)
```

What we are seeing here is the typical tension between using function closures as objects (the direct environment manipulation solution) or using objects as environments (the `wrapr` function object solution). The usual phrasings are "closures are poor man's objects" or "objects are poor man's closures."

The `wrapr::UnaryFn` pipeline is very easy to use (so called as the pipeline classes are derived from `wrapr::UnaryFn`).  Once you have set it up you pretty much want to use it (instead of repeating the steps by hand).  For example we can use it to evaluate our fit model performance on both training and test data.

```{r}

dTrain$prediction <- dTrain %.>% pipeline

WVPlots::ScatterHist(dTrain, "prediction", "y", "fit on training data",
                     smoothmethod = "identity",
                     estimate_sig = TRUE,
                     point_alpha = 0.1,
                     contour = TRUE)

dTest$prediction <- dTest %.>% pipeline

WVPlots::ScatterHist(dTest, "prediction", "y", "fit on test",
                     smoothmethod = "identity",
                     estimate_sig = TRUE,
                     point_alpha = 0.1,
                     contour = TRUE)
```

One can incorporate a `UnaryFn` into a larger [`rquery`](https://github.com/WinVector/rquery)/[`rqdatatable`](https://github.com/WinVector/rqdatatable/)  pipeline using the `rq_partial()` node, which takes a `UnaryFn` as an argument.  For example:


```{r rq}
library("rqdatatable")

# pipe line leaving result as a data.frame
# rquery/rqdatatable prefer to work over data.frames
# also show we can build pipelines without the pipe notation.
pipeline <- fnlist(list(
  pkgfn("vtreat::prepare",
        arg_name = "dframe", 
        args = list(treatmentplan = cp$treatments,
                    varRestriction = newvars)),
  pkgfn("subset",
        arg_name = "x",
        args = list(select = newvars)),
  pkgfn("scale",
        arg_name = "x",
        args = list(center = centering,
                    scale = scaling)),
  pkgfn("glmnet::predict.cv.glmnet",
        arg_name = "newx",
        args = list(object = model,
                    s = "lambda.1se"))))

cat(format(pipeline))



ops <- mk_td("d", colnames(dTrain)) %.>%
  rq_partial(., 
             step = pipeline,
             columns_produced = "1") 

cat(format(ops))

dTest %.>% ops %.>% head

head(dTest$prediction)
```

Obviously all we did is wrap the `UnaryFn` steps into an `rquery` node, but the point is that node could then be part of a larger non-trivial data processing pipeline (which also can be saved and shared).

And that is how to effectively save, share, and deploy non-trivial modeling workflows.


```{r cleanup}
parallel::stopCluster(cl)
```

